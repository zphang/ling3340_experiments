include "../../jiant/config/defaults.conf"

project_dir = ${JIANT_PROJECT_PREFIX}

pretrain_tasks = "cola"  // empty: don't run main training phase
target_tasks = "cola"   // train classifier only

do_pretrain = 1        // skip main train phase
do_target_task_training = 0  // train using eval task params
do_full_eval = 1


cuda = 0

classifier = mlp
classifier_hid_dim = 512
classifier_dropout = 0.2  // The dropout rate for mlp and fancy_mlp.
max_seq_len = 40
max_word_v_size = 30000  // Maximum input word vocab size, when creating a new embedding matrix. Not used for ELMo.

word_embs = "none"
char_embs = 0
d_word = 300
elmo = 0
elmo_chars_only = 0

tokenizer = "OpenAI.BPE"
openai_transformer = 1
openai_transformer_ckpt = ""
openai_embeddings_mode = "none"
openai_transformer_fine_tune = 1
sent_enc = "null"
sep_embs_for_skip = 1

bidirectional = 1
cola_classifier_hid_dim = 1024  // Hidden dimension size (usually num_heads * d_proj for transformer)
cola_d_proj = 1024
n_layers_enc = 2  // Number of encoder layers. Usually 8â€“12 for Transformer.
skip_embs = 1
dropout = 0.2  // Dropout rate.
lr = 0.0003


batch_size = 8
val_interval = 100
min_lr = 0.00001

write_preds = "val,test"