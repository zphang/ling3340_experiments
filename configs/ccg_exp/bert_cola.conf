include "../../../jiant/config/defaults.conf"



// _All_ final runs will share preproc and tasks.
bert_embeddings_mode = "top"   // how to use the outputs of the BERT module
                                // set as "top", we use only the top-layer activation
                                // other options: "only" uses the lexical layer (first layer)
                                //                "cat" uses lexical layer + top layer
bert_fine_tune = 1
elmo = 0
elmo_chars_only = 0
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
sep_embs_for_skip = 1
sent_enc="null"
classifier = log_reg // following BERT paper

dropout = 0.1 // following BERT paper
optimizer = bert_adam
max_epochs = 3
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

cola_lr = 0.00001
cola_val_interval = 1000


max_seq_len = 261
batch_size = 16


project_dir = ${JIANT_PROJECT_PREFIX}

elmo = 0
bert_fine_tune = "finetune"
transfer_paradigm = "finetune"

// tokenizer = "bert-large-uncased"
// bert_model_name = "bert-large-uncased"
// exp_name = "bert-large-uncased"

tokenizer = "bert-base-uncased"
bert_model_name = "bert-base-uncased"
exp_name = "bert-base-uncased"

pretrain_tasks = "cola"  // empty: don't run main training phase
target_tasks = "cola"   // train classifier only

do_pretrain = 1        // skip main train phase
do_target_task_training = 0  // train using eval task params
do_full_eval = 1

cuda = 0
batch_size = 24

write_preds = "val,test"